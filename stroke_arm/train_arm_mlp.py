# ì‹¤ì‹œê°„ MLP ê¸°ë°˜ ì–‘íŒ” ëŒ€ì¹­ ì¸¡ì • í”„ë¡œê·¸ë¨ (ìµœì¢… ë²„ì „)
import cv2
import numpy as np
import pandas as pd
import joblib
import time
from collections import Counter
import mediapipe as mp
import pyttsx3
from PIL import ImageFont, ImageDraw, Image

# ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©
model = joblib.load("model/mlp_arm_model.joblib")
scaler = joblib.load("model/scaler_arm.joblib")

# Mediapipe ì´ˆê¸°í™”
mp_pose = mp.solutions.pose
mp_hands = mp.solutions.hands
pose = mp_pose.Pose()
hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7, min_tracking_confidence=0.5)

# TTS ì´ˆê¸°í™”
tts = pyttsx3.init()
def speak(text):
    tts.say(text)
    tts.runAndWait()

# ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ì¶œë ¥
def draw_text(img, text, pos=(30, 40), size=26, color=(255, 255, 255)):
    try:
        font_path = "/System/Library/Fonts/Supplemental/AppleGothic.ttf"
        font = ImageFont.truetype(font_path, size)
    except:
        font = ImageFont.load_default()
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    draw.text(pos, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
def extract_arm_features(landmarks, hand_directions):
    def to_np(idx):
        return np.array([landmarks[idx].x, landmarks[idx].y, landmarks[idx].z])
    l_upper = to_np(13) - to_np(11)
    r_upper = to_np(14) - to_np(12)
    def vector_angle(v1, v2):
        cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)
        return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))
    angle_diff = vector_angle(l_upper, r_upper)
    depth_diff = abs(to_np(15)[2] - to_np(16)[2])
    height_diff = abs(to_np(15)[1] - to_np(16)[1])
    hand_dir = np.mean(hand_directions) if hand_directions else 0.0
    return [angle_diff, depth_diff, height_diff, hand_dir]

# 1ì´ˆê°„ ì •ìƒ ìœ ì§€ íŒë‹¨ì„ ìµœëŒ€ 10ì´ˆ ë™ì•ˆ 2íšŒ ì‹œë„
def wait_for_stable_pose_with_retry(cap, pose, hands, model, scaler, max_attempts=2, max_duration=10.0, interval=0.1, require_normal_count=10):
    for attempt in range(max_attempts):
        speak("ì–‘íŒ”ì„ ë“¤ì–´ì£¼ì„¸ìš”")
        print(f"\nğŸ•‘ ì‹œë„ {attempt + 1}: íŒ”ì„ ë“¤ì–´ì£¼ì„¸ìš”")
        stable_count = 0
        start_time = time.perf_counter()

        while time.perf_counter() - start_time < max_duration:
            ret, frame = cap.read()
            if not ret:
                continue
            frame = cv2.flip(frame, 1)
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pose_result = pose.process(rgb)
            hands_result = hands.process(rgb)

            hand_dirs = []
            lh_finger_coords = [0.0] * 63
            rh_finger_coords = [0.0] * 63

            if hands_result.multi_hand_landmarks and hands_result.multi_handedness:
                for idx, hand_landmarks in enumerate(hands_result.multi_hand_landmarks):
                    handedness = hands_result.multi_handedness[idx].classification[0].label
                    wrist = hand_landmarks.landmark[0]
                    index_tip = hand_landmarks.landmark[8]
                    dx = index_tip.x - wrist.x
                    dy = index_tip.y - wrist.y
                    direction = np.arctan2(dy, dx)
                    hand_dirs.append(direction)
                    coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y, lm.z)]
                    if handedness == 'Left':
                        lh_finger_coords = coords
                    else:
                        rh_finger_coords = coords

            if pose_result.pose_landmarks:
                try:
                    landmarks = pose_result.pose_landmarks.landmark
                    features = extract_arm_features(landmarks, hand_dirs)
                    input_features = features + lh_finger_coords + rh_finger_coords
                    X_input = pd.DataFrame([input_features], columns=scaler.feature_names_in_)
                    X_scaled = scaler.transform(X_input)
                    pred = model.predict(X_scaled)[0]
                    if pred == 0:
                        stable_count += 1
                        if stable_count >= require_normal_count:
                            speak("íŒ”ì´ ì•ˆì •ì ìœ¼ë¡œ ì¸ì‹ë˜ì—ˆìŠµë‹ˆë‹¤. ì¸¡ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                            return True
                    else:
                        stable_count = 0
                except:
                    stable_count = 0

            frame = draw_text(frame, f"ì •ìƒ ìœ ì§€ ì¤‘: {stable_count}/10", color=(0, 255, 0))
            cv2.imshow("ì‹¤ì‹œê°„ ì˜ˆì¸¡", frame)
            if cv2.waitKey(1) == 27:
                return False
            time.sleep(interval)

        if attempt < max_attempts - 1:
            speak("íŒ”ì´ ì•ˆì •ì ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ íŒ”ì„ ë“¤ì–´ì£¼ì„¸ìš”.")

    speak("ì¸¡ì •ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    return False

# 5ì´ˆê°„ 50í”„ë ˆì„ ì¸¡ì •

def perform_measurement(cap, pose, hands, model, scaler, duration=5.0, interval=0.1):
    results = []
    for _ in range(50):  # 50íšŒë¡œ ì œí•œ
        iter_start = time.perf_counter()
        ret, frame = cap.read()
        if not ret:
            continue
        frame = cv2.flip(frame, 1)
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pose_result = pose.process(rgb)
        hands_result = hands.process(rgb)

        hand_dirs = []
        lh_finger_coords = [0.0] * 63
        rh_finger_coords = [0.0] * 63

        if hands_result.multi_hand_landmarks and hands_result.multi_handedness:
            for idx, hand_landmarks in enumerate(hands_result.multi_hand_landmarks):
                handedness = hands_result.multi_handedness[idx].classification[0].label
                wrist = hand_landmarks.landmark[0]
                index_tip = hand_landmarks.landmark[8]
                dx = index_tip.x - wrist.x
                dy = index_tip.y - wrist.y
                direction = np.arctan2(dy, dx)
                hand_dirs.append(direction)
                coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y, lm.z)]
                if handedness == 'Left':
                    lh_finger_coords = coords
                else:
                    rh_finger_coords = coords

        if pose_result.pose_landmarks:
            try:
                landmarks = pose_result.pose_landmarks.landmark
                features = extract_arm_features(landmarks, hand_dirs)
                input_features = features + lh_finger_coords + rh_finger_coords
                X_input = pd.DataFrame([input_features], columns=scaler.feature_names_in_)
                X_scaled = scaler.transform(X_input)
                pred = model.predict(X_scaled)[0]
                results.append(pred)
                frame = draw_text(frame, f"ì¸¡ì • ì¤‘... {len(results)}/50", color=(0, 255, 255))
            except:
                pass

        cv2.imshow("ì‹¤ì‹œê°„ ì˜ˆì¸¡", frame)
        if cv2.waitKey(1) == 27:
            break
        elapsed = time.perf_counter() - iter_start
        time.sleep(max(0.0, interval - elapsed))
    return results

# ì¸¡ì • í‰ê°€
def evaluate_and_report(results, round_num=1):
    counter = Counter(results)
    normal_count = counter[0]
    total = len(results)
    rate = (normal_count / total) * 100
    print(f"\nğŸ“Š [{round_num}ì°¨ ì¸¡ì •] íŒ” ëŒ€ì¹­ë¥ : {rate:.2f}% (ì •ìƒ {normal_count} / ì´ {total})")
    speak(f"íŒ” ëŒ€ì¹­ë¥ ì€ {int(rate)} í¼ì„¼íŠ¸ì…ë‹ˆë‹¤.")
    return rate

# ë©”ì¸ ë£¨í”„
def main():
    cap = cv2.VideoCapture(0)
    if wait_for_stable_pose_with_retry(cap, pose, hands, model, scaler):
        results1 = perform_measurement(cap, pose, hands, model, scaler)
        rate1 = evaluate_and_report(results1)
        if rate1 >= 80:
            speak("íŒ”ì´ ì •ìƒì ìœ¼ë¡œ ëŒ€ì¹­ì…ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            cap.release()
            cv2.destroyAllWindows()
            return
        else:
            speak("íŒ”ì´ ë¹„ëŒ€ì¹­ì´ ì˜ì‹¬ë©ë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ì¸¡ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            if wait_for_stable_pose_with_retry(cap, pose, hands, model, scaler):
                results2 = perform_measurement(cap, pose, hands, model, scaler)
                rate2 = evaluate_and_report(results2, round_num=2)
                if rate2 >= 80:
                    speak("ë‘ ë²ˆì§¸ ì¸¡ì • ê²°ê³¼, íŒ”ì´ ì •ìƒì ìœ¼ë¡œ ëŒ€ì¹­ì…ë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                else:
                    speak("íŒ”ì´ ë¹„ëŒ€ì¹­ì´ ì˜ì‹¬ë˜ë¯€ë¡œ ê°€ê¹Œìš´ ë³‘ì›ì— ë‚´ì› ë¶€íƒë“œë¦½ë‹ˆë‹¤.")
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
